{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision-hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Directory\n",
    "\n",
    "    - magic_Yolov4_turtle_tiny_sub4_newdata.ipynb \n",
    "    - detect.ipynb\n",
    "    - move-2.txt\n",
    "    - turtle_test3.py\n",
    "    - screen_grab.ipynb\n",
    "    - re-size-for-HD.ipynb\n",
    "    - convert_class_of_txt.ipynb\n",
    "    - file_structure-yoloV4.ipynb\n",
    "    - Balance_classes.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guestures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine Guestures based on sign language where selected to tied to input commands, for the MVP.\n",
    "This set of nine basic commands where mapped to different action to have a stronger visual impat with out MVP using turtle python.  \n",
    " - foward -> foward \n",
    " - back   -> back\n",
    " - left -> star \n",
    " - right -> octagon\n",
    " - input -> standby (tell the system a command is about to be entered)\n",
    " - plus -> orbe ( join commads together to form a series of commands)\n",
    " - three -> hexigon\n",
    " - seven -> magic\n",
    " - five -> run\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this project I have a new understanding on how important data is.  The understanding that a model is as good as the data it eats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to data: https://drive.google.com/file/d/18E5KIMsRYFHx9XOUkHj_UsItmLDCuYJ8/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources:\n",
    "- Images from David Lee's database on Roboflow\n",
    "- community sourced images for ~ 25 individuals via phone photos \n",
    "- webcam captures (16:9)\n",
    "- NAN image (images not of the classes without bounding boxes)\n",
    "- In total ~ 1,200 pieces of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data Workflow:\n",
    "- collect data\n",
    "- resize data to something smaller then the ~3mb JPEG of normal phones\n",
    "- when testing particular augmentations we would transoform the image in via photoshop and bridge software\n",
    "- images would be inputed into cvat.org for labeling, where a bounding box would be placed on the image\n",
    "- image would be exported in a darkframe annotated format to continue down the pipe line to be fed directly not the model\n",
    "- for batch images the files would be exported as annotated pascal voc to be fed into roboflow for augmentation\n",
    "- within roboflow the image would be augmented and exported into darknet annotated format\n",
    "- images would be combined with other datasetes, and would be zipped together and prepared for yolo model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating more data by altering the origonal data through: geometric transformations, flipping, color modification, cropping, rotation, and etc.\n",
    "- conversion to black and white\n",
    "- rotation\n",
    "- color hue\n",
    "- size (uneffective if over used)\n",
    "- brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect ratio:\n",
    "- The model will reduce the shape of the input image into a square. \n",
    "- we theorized that this compression effects the shape of the scanned image. \n",
    "    - a vertical phone image compressed into square would create a shot and fat shape.\n",
    "    - a webcam (16:9) aspect ratio would create a tall and skinny image when compressed.\n",
    "- We experimented by adding black canves to the vertical iphone images to create the HD perportions when they get compressed to 1:1 square.  \n",
    "- Performence dropped when this was done because we feel that there was too much added black that did not work well with the bag of tricks built in augmentation within the begining of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data conclusions:\n",
    "It is necessary to be very organized with your workflow when dealing in huge volumes of image data.  A small about of mislabeled classes can cause catastrophic results.\n",
    "Data augmentation is a necessity in most situation, the model performed much better once more data was fed to it through augmentation.  All augmentation are not created equal depending on the augmentation the performance may drop.  For example ,the model did not perform as well when the data brought in from the webcam was appyled to a brightness augmentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloV4 tiny on Darknet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YoloV4 was chooses as the arcutecture for the computer vision model because it is fast, opensource, very accurate.  \n",
    "\n",
    "- yoloV4 darknet is an open source neural network framework written in C and CUDA.\n",
    "\n",
    "- yoloV4 is the most uptodate model of yolo and daknet was the origonal framework that it was built on, it has latter been adapted for tensorflow, pytorch and ......\n",
    "\n",
    "- the tiny model was used because it is able to run on a cpu at close to real time speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google colab was utilized to train the model because of its processing speeds far surpassed my PC\n",
    "- the darknet framework was downloaded onto my computer then cloned to my github\n",
    " - https://github.com/encodingintuition/darknet-yolov4_tiny\n",
    "- This allowed me to edit the architecture file and push changes up instead of manipuating files through the colab interface\n",
    " - the architecture file is   *yolov4_tiny_train.cfg*,   and be a few examples of adaptions are saved in the architecture folder\n",
    "- Training would take 5 - 7 hours though colab and the created custom weights would be saved to google drive\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Conclusions \n",
    "\n",
    "- The recipe that yielded the highest results was a subdivisions 4, a momentum of 0.949, leaky relu as activations, and a learning rate of .001.  Surprisingly there was a dramatic crash of performance when the learning rate was reduced to 0.0001.\n",
    "\n",
    "#### Performence\n",
    "- Our mAP performence in the end was very high at 92.96 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCv was utilized to run object detection through a live webcam -  *detect.ipynb* \n",
    "- The best weights and model architecture are loaded into the library\n",
    "- The function *turtle_talk* function was written that read the detected class in from the model and write the corresponding number to ,*move-2.txt*, an external file.  The file *turtle-test3.py* reads in the integer in the *move-2.txt* and executes the corresponding action within the turtle screen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| class          | Average Precision    | TP    |  FP |\n",
    "| :------------- | :----------: | :----------: |-----------: |\n",
    "|  foward | 90.00  | 9   | 2|\n",
    "|  back | 100  | 10  | 0|\n",
    "|  left| 83.33  | 5   | 0|\n",
    "|  right| 91.82  | 10   | 2|\n",
    "|  input| 90  | 5   | 1|\n",
    "|  plus| 81.48  | 8   | 1|\n",
    "|  three| 100  | 5   | 2|\n",
    "|  seven | 100  | 7   | 1|\n",
    "|five | 100| 2 |0 \\||\n",
    "\n",
    "| Average mean precision (mAP)         |recall    | precision   | F1-score |\n",
    "| :------------- | :----------: | :----------: |-----------: |\n",
    "|  92.96 | 0.91  | 0.87   | 0.89|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref:\n",
    "- Understanding data Augmentation: https://github.com/AgaMiko/data-augmentation-review\n",
    "- alexey ab repro https://github.com/AlexeyAB\n",
    "- David Lee's dataset on roboflow https://public.roboflow.com/object-detection/american-sign-language-letters\n",
    "- Roboflow for data augmentation https://https://roboflow.com/\n",
    "- google colob for their servers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
