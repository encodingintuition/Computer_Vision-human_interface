{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opportunity Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a more human interface to interact with a computer system.\n",
    "\n",
    "- Stage 1 – to have a computer system interact with pre-defined gestures \n",
    "- Stage 2 – to have a model intuit human needs through our gestures ( as the family dog uses observation to reactively communicate) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Goal: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train a convolutional neural network to identify particular hand gestures.\n",
    "- To have these hand guestures trigure actions within another python application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- opensource online archive \n",
    "- ~15 partisipants via phone photos of their hands\n",
    "- webcam screen shoot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| class          | Average Precision    | TP    |  FP |\n",
    "| :------------- | :----------: | :----------: |-----------: |\n",
    "|  foward | 90.00  | 9   | 2|\n",
    "|  back | 100  | 10  | 0|\n",
    "|  left| 83.33  | 5   | 0|\n",
    "|  right| 91.82  | 10   | 2|\n",
    "|  input| 90  | 5   | 1|\n",
    "|  plus| 81.48  | 8   | 1|\n",
    "|  three| 100  | 5   | 2|\n",
    "|  seven | 100  | 7   | 1|\n",
    "|five | 100| 2 |0 \\||\n",
    "\n",
    "| Average mean precision (mAP)         |recall    | precision   | F1-score |\n",
    "| :------------- | :----------: | :----------: |-----------: |\n",
    "|  92.96 | 0.91  | 0.87   | 0.89|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A YoloV4 model is successful in being able to identify particular hand gestures after being trained on custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detection is able to take place in real time and because of the architecture \n",
    "of the YoloV4 model there is not a significant computational cost put on hardware. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Test different element's of the data in small batches and compare mAP.  Isolating:  angle of view & rotation\n",
    "- creating better tools to manage data workflow\n",
    "- Look into other models such as Mediapipe (trained on 30K hands)\n",
    "- Look into passively collecting data, ‘clustering’ for body movements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Test different element's of the data in small batches and compare mAP.  Isolating:  angle of view & rotation\n",
    "- creating better tools to manage data workflow\n",
    "- Look into other models such as Mediapipe (trained on 30K hands)\n",
    "- Look into passively collecting data, ‘clustering’ for body movements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community notes\n",
    "- Below are notes from the yolo community that are most helpful when creating a model. \n",
    "- One of the greatest strengths of the yolo model is the Opensource community that supports it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About YoloV4 tiny "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AlexeyAB/darknet/issues/6067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People to reconize in the comunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yashas Samaga B L\n",
    "\n",
    "Worked to create YOLOv4 inference using OpenCV DNN \n",
    "\n",
    "https://gist.github.com/YashasSamaga/e2b19a6807a13046e399f4bc3cca3a49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-%5Bnet%5D-section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are notes from   https://github.com/AlexeyAB/darknet#How-to-improve-object-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check that each object that you want to detect is mandatory labeled in your dataset - no one object in your data set should not be without label. In the most training issues - there are wrong labels in your dataset (got labels by using some conversion script, marked with a third-party tool, ...). Always check your dataset by using: https://github.com/AlexeyAB/Yolo_mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each object which you want to detect - there must be at least 1 similar object in the Training dataset with about the same: shape, side of object, relative size, angle of rotation, tilt, illumination. So desirable that your training dataset include images with objects at diffrent: scales, rotations, lightings, from different sides, on different backgrounds - you should preferably have 2000 different images for each class or more, and you should train 2000*classes iterations or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- desirable that your training dataset include images with non-labeled objects that you do not want to detect - negative samples without bounded box (empty .txt files) - use as many images of negative samples as there are images with objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you train the model to distinguish Left and Right objects as separate classes (left/right hand, left/right-turn on road signs, ...) then for disabling flip data augmentation - add flip=0 here: https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- General rule - your training dataset should include such a set of relative sizes of objects that you want to detect:\n",
    "\n",
    "    train_network_width * train_obj_width / train_image_width ~= detection_network_width * detection_obj_width / detection_image_width\n",
    "    train_network_height * train_obj_height / train_image_height ~= detection_network_height * detection_obj_height / detection_image_height\n",
    "\n",
    "I.e. for each object from Test dataset there must be at least 1 object in the Training dataset with the same class_id and about the same relative size:\n",
    "\n",
    "object width in percent from Training dataset ~= object width in percent from Test dataset\n",
    "\n",
    "That is, if only objects that occupied 80-90% of the image were present in the training set, then the trained network will not be able to detect objects that occupy 1-10% of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each: model of object, side, illimination, scale, each 30 grad of the turn and inclination angles - these are different objects from an internal perspective of the neural network. So the more different objects you want to detect, the more complex network model should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to make the detected bounded boxes more accurate, you can add 3 parameters ignore_thresh = .9 iou_normalizer=0.5 iou_loss=giou to each [yolo] layer and train, it will increase mAP@0.9, but decrease mAP@0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training - for detection:\n",
    "\n",
    "- Increase network-resolution by set in your .cfg-file (height=608 and width=608) or (height=832 and width=832) or (any value multiple of 32) - this increases the precision and makes it possible to detect small objects: link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About anchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/pjreddie/darknet/issues/911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only if you are an expert in neural detection networks - recalculate anchors for your dataset for width and height from cfg-file: darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416 then set the same 9 anchors in each of 3 [yolo]-layers in your cfg-file. But you should change indexes of anchors masks= for each [yolo]-layer, so for YOLOv4 the 1st-[yolo]-layer has anchors smaller than 30x30, 2nd smaller than 60x60, 3rd remaining, and vice versa for YOLOv3. Also you should change the filters=(classes + 5)*<number of mask> before each [yolo]-layer. If many of the calculated anchors do not fit under the appropriate layers - then just try using all the default anchors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mosaic Augmentation Paper? #8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WongKinYiu/CrossStagePartialNetworks/issues/8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
